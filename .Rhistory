print(head(data))
# Step 1: Read the XLSX file
data <- read_excel("Midterm_Dataset_Section(C).xlsx")
print("Data loaded:")
print(head(data))
library(readxl)
library(dplyr)
library(ggplot2)
library(naniar)
data <- read_excel("Midterm_Dataset_Section(C).xlsx")
print("Data loaded:")
print(head(data))
summary(data)
total_missing <- sum(is.na(data))
print(paste("Total missing values across the dataset:", total_missing))
missing_counts <- sapply(data, function(x) sum(is.na(x)))
print("Missing values per column:")
print(missing_counts)
get_mode <- function(v) {
uniqv <- unique(na.omit(v))
uniqv[which.max(tabulate(match(v, uniqv)))]
}
income_lower_bound <- 50000
income_upper_bound <- 500000
data <- data %>%
filter(person_income >= income_lower_bound & person_income <= income_upper_bound)
print("Data after filtering by income range:")
print(head(data))
age_lower_bound <- 20
age_upper_bound <- 80
data <- data %>%
filter(person_age >= age_lower_bound & person_age <= age_upper_bound)
print("Data after filtering by Age:")
print(head(data))
data$person_age[is.na(data$person_age)] <- mean(data$person_age, na.rm = TRUE)
data$person_income[is.na(data$person_income)] <- median(data$person_income, na.rm = TRUE)
print("Data after imputing numeric columns:")
print(head(data))
mode_education <- get_mode(data$person_education)
data$person_education[is.na(data$person_education)] <- mode_education
data$person_education <- as.factor(data$person_education)
mode_loan_status <- get_mode(data$loan_status)
data$loan_status[is.na(data$loan_status)] <- mode_loan_status
data$loan_status <- as.factor(data$loan_status)
mode_gender <- get_mode(data$person_gender)
data$person_gender[is.na(data$person_gender)] <- mode_gender
data$person_gender <- as.factor(data$person_gender)
print("Data after imputing and converting categorical columns:")
print(head(data))
data <- data %>%
distinct()
print("Data after removing duplicates:")
print(head(data))
data$normalized_person_income <- (data$person_income - min(data$person_income)) / (max(data$person_income) - min(data$person_income))
print("Data after normalization of person_income:")
print(head(data))
summary_stats <- summary(data)
print("Summary statistics of dataset:")
print(summary_stats)
numeric_columns <- sapply(data, is.numeric)
std_devs <- sapply(data[, numeric_columns, drop = FALSE], sd, na.rm = TRUE)
print("Standard deviations of numeric columns:")
print(std_devs)
data <- na.omit(data)
print(head(data))
std_data <- data.frame(Variable = names(std_devs), StdDev = std_devs)
ggplot(std_data, aes(x = Variable, y = StdDev)) +
geom_col(fill = "blue") +
theme_minimal() +
labs(title = "Standard Deviation of Numeric Columns", x = "Variable", y = "Standard Deviation")
Q1 <- quantile(data$person_income, 0.25, na.rm = TRUE)
Q3 <- quantile(data$person_income, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
outliers <- data$person_income < lower_bound | data$person_income > upper_bound
print("Visualizing outliers in person_income:")
ggplot(data, aes(x = person_income)) +
geom_histogram(fill = "blue", color = "black", binwidth = 500) +
geom_vline(xintercept = c(lower_bound, upper_bound), color = "red", linetype = "dashed", size = 1) +
labs(title = "Person Income with Outlier Thresholds")
data <- data[!outliers, ]
print("Data after removing outliers:")
print(head(data))
ggplot(data, aes(x = normalized_person_income)) +
geom_histogram(bins = 30, fill = "red", color = "black") +
theme_minimal() +
labs(title = "Distribution of Normalized Person Income", x = "Normalized Income", y = "Frequency")
print("Final cleaned data:")
print(head(data))
write.csv(data, "cleaned_dataset.csv", row.names = FALSE)
print("Cleaned dataset exported to 'cleaned_dataset.csv' successfully!")
View(std_data)
View(get_mode)
View(get_mode)
total_missing <- sum(is.na(data))
print(paste("Total missing values across the dataset:", total_missing))
library(readxl)
library(dplyr)
library(ggplot2)
library(naniar)
data <- read_excel("Midterm_Dataset_Section(C).xlsx")
print("Data loaded:")
print(head(data))
summary(data)
total_missing <- sum(is.na(data))
print(paste("Total missing values across the dataset:", total_missing))
missing_counts <- sapply(data, function(x) sum(is.na(x)))
print("Missing values per column:")
print(missing_counts)
get_mode <- function(v) {
uniqv <- unique(na.omit(v))
uniqv[which.max(tabulate(match(v, uniqv)))]
}
income_lower_bound <- 50000
income_upper_bound <- 500000
data <- data %>%
filter(person_income >= income_lower_bound & person_income <= income_upper_bound)
print("Data after filtering by income range:")
print(head(data))
age_lower_bound <- 20
age_upper_bound <- 80
data <- data %>%
filter(person_age >= age_lower_bound & person_age <= age_upper_bound)
print("Data after filtering by Age:")
print(head(data))
data$person_age[is.na(data$person_age)] <- mean(data$person_age, na.rm = TRUE)
data$person_income[is.na(data$person_income)] <- median(data$person_income, na.rm = TRUE)
print("Data after imputing numeric columns:")
print(head(data))
mode_education <- get_mode(data$person_education)
data$person_education[is.na(data$person_education)] <- mode_education
data$person_education <- as.factor(data$person_education)
mode_loan_status <- get_mode(data$loan_status)
data$loan_status[is.na(data$loan_status)] <- mode_loan_status
data$loan_status <- as.factor(data$loan_status)
mode_gender <- get_mode(data$person_gender)
data$person_gender[is.na(data$person_gender)] <- mode_gender
data$person_gender <- as.factor(data$person_gender)
print("Data after imputing and converting categorical columns:")
print(head(data))
data <- data %>%
distinct()
print("Data after removing duplicates:")
print(head(data))
data$normalized_person_income <- (data$person_income - min(data$person_income)) / (max(data$person_income) - min(data$person_income))
print("Data after normalization of person_income:")
print(head(data))
summary_stats <- summary(data)
print("Summary statistics of dataset:")
print(summary_stats)
numeric_columns <- sapply(data, is.numeric)
std_devs <- sapply(data[, numeric_columns, drop = FALSE], sd, na.rm = TRUE)
print("Standard deviations of numeric columns:")
print(std_devs)
data <- na.omit(data)
print(head(data))
std_data <- data.frame(Variable = names(std_devs), StdDev = std_devs)
ggplot(std_data, aes(x = Variable, y = StdDev)) +
geom_col(fill = "blue") +
theme_minimal() +
labs(title = "Standard Deviation of Numeric Columns", x = "Variable", y = "Standard Deviation")
Q1 <- quantile(data$person_income, 0.25, na.rm = TRUE)
Q3 <- quantile(data$person_income, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
outliers <- data$person_income < lower_bound | data$person_income > upper_bound
print("Visualizing outliers in person_income:")
ggplot(data, aes(x = person_income)) +
geom_histogram(fill = "blue", color = "black", binwidth = 500) +
geom_vline(xintercept = c(lower_bound, upper_bound), color = "red", linetype = "dashed", size = 1) +
labs(title = "Person Income with Outlier Thresholds")
data <- data[!outliers, ]
print("Data after removing outliers:")
print(head(data))
ggplot(data, aes(x = normalized_person_income)) +
geom_histogram(bins = 30, fill = "red", color = "black") +
theme_minimal() +
labs(title = "Distribution of Normalized Person Income", x = "Normalized Income", y = "Frequency")
print("Final cleaned data:")
print(head(data))
write.csv(data, "cleaned_dataset.csv", row.names = FALSE)
print("Cleaned dataset exported to 'cleaned_dataset.csv' successfully!")
if (!require("rvest")) install.packages("rvest")
if (!require("tm")) install.packages("tm")
if (!require("SnowballC")) install.packages("SnowballC")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("textstem")) install.packages("textstem")
if (!require("hunspell")) install.packages("hunspell")
library(rvest)
library(tm)
library(SnowballC)
library(tidyverse)
library(textstem)
library(hunspell)
url <- 'https://stackoverflow.com/questions'
page <- read_html(url)
text_data <- page %>% html_nodes('body') %>% html_text() %>% .[1]
cat("Original scraped text:\n", text_data, "\n\n")
clean_text <- function(text) {
text <- gsub("<[^>]+>", "", text)
text <- gsub("[[:punct:]]", " ", text)
text <- gsub("\\s+", " ", text)
text <- tolower(text)
return(text)
}
cleaned_text <- clean_text(text_data)
cat("Text after cleaning:\n", cleaned_text, "\n\n")
tokenized_text <- strsplit(cleaned_text, "\\s+")[[1]]
cat("Text after tokenization:\n", toString(tokenized_text), "\n\n")
stopwords_list <- stopwords("en")
cleaned_tokens <- tokenized_text[!tokenized_text %in% stopwords_list]
cat("Text after removing stopwords:\n", toString(cleaned_tokens), "\n\n")
stem_tokens <- function(tokens) {
sapply(tokens, stemDocument, language = "english")
}
stemmed_text <- stem_tokens(cleaned_tokens)
cat("Text after stemming:\n", toString(stemmed_text), "\n\n")
lemmatize_tokens <- function(tokens) {
lapply(tokens, lemmatize_words)
}
lemmatized_text <- unlist(lemmatize_tokens(list(stemmed_text)))
cat("Text after lemmatization:\n", toString(lemmatized_text), "\n\n")
remove_emojis <- function(text) {
gsub("[^\x01-\x7F]", "", text)
}
text_no_emojis <- remove_emojis(cleaned_text)
cat("Text after removing emojis and special characters:\n", text_no_emojis, "\n\n")
correct_spelling <- function(tokens) {
corrections <- sapply(tokens, function(token) {
suggestions <- hunspell::hunspell_suggest(token)
if (length(suggestions[[1]]) > 0) suggestions[[1]][1] else token
})
return(corrections)
}
corrected_text <- correct_spelling(stemmed_text)
cat("Text after spell checking:\n", toString(corrected_text), "\n\n")
output_file <- "H:/Theory/DS/Mid_Project_DataScience/cleaned_text.txt"
writeLines(cleaned_text, output_file)
cat("Cleaned text has been exported to", output_file, "\n")
url <- 'https://stackoverflow.com/questions'
page <- read_html(url)
text_data <- page %>% html_nodes('body') %>% html_text() %>% .[1]
cat("Original scraped text:\n", text_data, "\n\n")
clean_text <- function(text) {
text <- gsub("<[^>]+>", "", text)
text <- gsub("[[:punct:]]", " ", text)
text <- gsub("\\s+", " ", text)
text <- tolower(text)
return(text)
}
cleaned_text <- clean_text(text_data)
cat("Text after cleaning:\n", cleaned_text, "\n\n")
tokenized_text <- strsplit(cleaned_text, "\\s+")[[1]]
cat("Text after tokenization:\n", toString(tokenized_text), "\n\n")
stopwords_list <- stopwords("en")
cleaned_tokens <- tokenized_text[!tokenized_text %in% stopwords_list]
cat("Text after removing stopwords:\n", toString(cleaned_tokens), "\n\n")
stem_tokens <- function(tokens) {
sapply(tokens, stemDocument, language = "english")
}
stemmed_text <- stem_tokens(cleaned_tokens)
cat("Text after stemming:\n", toString(stemmed_text), "\n\n")
lemmatize_tokens <- function(tokens) {
lapply(tokens, lemmatize_words)
}
lemmatized_text <- unlist(lemmatize_tokens(list(stemmed_text)))
cat("Text after lemmatization:\n", toString(lemmatized_text), "\n\n")
remove_emojis <- function(text) {
gsub("[^\x01-\x7F]", "", text)
}
text_no_emojis <- remove_emojis(cleaned_text)
cat("Text after removing emojis and special characters:\n", text_no_emojis, "\n\n")
remove_emojis <- function(text) {
gsub("[^\x01-\x7F]", "1", text)
}
text_no_emojis <- remove_emojis(cleaned_text)
cat("Text after removing emojis and special characters:\n", text_no_emojis, "\n\n")
remove_emojis <- function(text) {
gsub("[^\x01-\x7F]", "RATTTTT", text)
}
text_no_emojis <- remove_emojis(cleaned_text)
cat("Text after removing emojis and special characters:\n", text_no_emojis, "\n\n")
correct_spelling <- function(tokens) {
corrections <- sapply(tokens, function(token) {
suggestions <- hunspell::hunspell_suggest(token)
if (length(suggestions[[1]]) > 0) suggestions[[1]][1] else token
})
return(corrections)
}
corrected_text <- correct_spelling(stemmed_text)
cat("Text after spell checking:\n", toString(corrected_text), "\n\n")
output_file <- "H:/Theory/DS/Mid_Project_DataScience/cleaned_text.txt"
writeLines(cleaned_text, output_file)
cat("Cleaned text has been exported to", output_file, "\n")
library(rvest)
library(tm)
library(SnowballC)
library(tidyverse)
library(textstem)
library(hunspell)
contractions_dict <- c(
"I'm" = "I am",
"you're" = "you are",
"he's" = "he is",
"she's" = "she is",
"it's" = "it is",
"we're" = "we are",
"they're" = "they are",
"don't" = "do not",
"doesn't" = "does not",
"can't" = "cannot",
"won't" = "will not",
"didn't" = "did not"
)
# Function to expand contractions
expand_contractions <- function(text, contractions_dict) {
for (contraction in names(contractions_dict)) {
text <- gsub(contraction, contractions_dict[contraction], text, ignore.case = TRUE)
}
return(text)
}
url <- 'https://stackoverflow.com/questions'
page <- read_html(url)
text_data <- page %>% html_nodes('body') %>% html_text() %>% .[1]
cat("Original scraped text:\n", text_data, "\n\n")
clean_text <- function(text) {
text <- gsub("<[^>]+>", "", text)
text <- gsub("[[:punct:]]", " ", text)
text <- gsub("\\s+", " ", text)
text <- tolower(text)
return(text)
}
cleaned_text <- clean_text(text_data)
cat("Text after cleaning:\n", cleaned_text, "\n\n")
tokenized_text <- strsplit(cleaned_text, "\\s+")[[1]]
cat("Text after tokenization:\n", toString(tokenized_text), "\n\n")
stopwords_list <- stopwords("en")
cleaned_tokens <- tokenized_text[!tokenized_text %in% stopwords_list]
cat("Text after removing stopwords:\n", toString(cleaned_tokens), "\n\n")
stem_tokens <- function(tokens) {
sapply(tokens, stemDocument, language = "english")
}
stemmed_text <- stem_tokens(cleaned_tokens)
cat("Text after stemming:\n", toString(stemmed_text), "\n\n")
lemmatize_tokens <- function(tokens) {
lapply(tokens, lemmatize_words)
}
lemmatized_text <- unlist(lemmatize_tokens(list(stemmed_text)))
cat("Text after lemmatization:\n", toString(lemmatized_text), "\n\n")
text_with_contractions <- expand_contractions(toString(lemmatized_text), contractions_dict)
cat("Text after handling contractions:\n", text_with_contractions, "\n\n")
remove_emojis <- function(text) {
gsub("[^\x01-\x7F]", "", text)
}
text_no_emojis <- remove_emojis(text_with_contractions)
cat("Text after removing emojis and special characters:\n", text_no_emojis, "\n\n")
correct_spelling <- function(tokens) {
corrections <- sapply(tokens, function(token) {
suggestions <- hunspell::hunspell_suggest(token)
if (length(suggestions[[1]]) > 0) suggestions[[1]][1] else token
})
return(corrections)
}
corrected_text <- correct_spelling(stemmed_text)
cat("Text after spell checking:\n", toString(corrected_text), "\n\n")
output_file <- "H:/Theory/DS/Mid_Project_DataScience/cleaned_text.txt"
writeLines(cleaned_text, output_file)
cat("Cleaned text has been exported to", output_file, "\n")
